% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/categorical_specific_agreement.R
\name{cat_specific}
\alias{cat_specific}
\title{Calculate Specific Agreement Coefficients}
\usage{
cat_specific(.data, categories = NULL, bootstrap = 2000,
  interval = 0.95, digits = 3, warnings = TRUE)
}
\arguments{
\item{.data}{\emph{Required.} An matrix or data frame where rows correspond to
objects of measurement, columns correspond to sources of measurement (e.g.,
raters), and values correspond to category assignments. Note that \code{NA}
will be treated as missing values, so if \code{NA} is a meaningful code in
your data, then recode them to another value and reserve \code{NA} for
missing values.}

\item{categories}{\emph{Optional.} A numeric vector of possible categories. Useful
when \code{.data} may not contain all possible categories.}

\item{bootstrap}{\emph{Optional.} A positive integer specifying how many bootstrap
resamples to use in calculating the confidence intervals (default = 2000).
Or, to prevent bootstrapping, set to \code{NULL}.}

\item{interval}{\emph{Optional.} A real number greater than 0 and less than 1
specifying the width of the confidence interval to calculate such as 0.95
for a 95\% confidence interval (default = 0.95).}

\item{digits}{\emph{Optional.} A positive integer specifying how many digits to
round the specific agreement estimates to (default = 3). Or, to prevent
rounding, set to \code{NULL}.}

\item{warnings}{\emph{Optional.} A logical specifying whether to include warnings
(default = TRUE).}
}
\value{
A tibble containing two or four columns and one row for each
category. The first variable is "Category" and contains the name of each
category. The second variable is "Agreement" and contains the estimated
specific agreement coefficient for each category. If \code{bootstrap} is
not NULL, the third variable is "Agreement_LCI" and contains the lower
bound of the bootstrap confidence interval and the the fourth variables is
"Agreement_UCI" and contains the upper bound of the bootstrap confidence
interval.
}
\description{
Calculate the percent observed agreement for each discrete category in an
object-by-rater matrix. Specific agreement is an index of the reliability of
categorical measurements. It describes the amount of agreement observed with
regard to specific categories. Thus, multiple specific agreement scores are
typically used (i.e., one for each category). With two raters, the
interpretation of specific agreement for any category is the probability of
one rater assigning an item to that category given that the other rater has
also assigned that item to that category. With more than two raters, the
interpretation becomes the probability of a randomly chosen rater assigning
an item to that category given that another randomly chosen rater has also
assigned that item to that category. When applied to binary (i.e.,
dichotomous) tasks, specific agreement on the positive category is often
referred to as positive agreement (PA) and specific agreement on the negative
category is often referred to as negative agreement (NA). It is also worth
noting that positive agreement (PA) for two raters is equivalent to the F1
score commonly used in computer science.
}
\examples{
data("ordered")
cat_specific(ordered, categories = 0:3)
}
\references{
Uebersax, J. S. (1982). A design-independent method for measuring
the reliability of psychiatric diagnosis. \emph{Journal of Psychiatric Research,
17}(4), 335-342. \url{https://doi.org/10/fbbdfn}
}
\concept{agreement functions for categorical data}
