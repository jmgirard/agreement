---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
set.seed(11)
```
# agreement

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
<!-- badges: end -->

The goal of the `agreement` package is to calculate estimates of inter-rater agreement and reliability using generalized formulas that accommodate different designs (e.g., crossed or uncrossed), missing data, and ordered or unordered categories. The package includes generalized functions for all major chance-adjusted indexes of categorical agreement (i.e., &alpha;, &gamma;, Ir2, &kappa;, &pi;, and S) as well as all major intraclass correlation coefficients (i.e., one-way and two-way models, agreement and consistency types, and single measure and average measure units). Estimates include bootstrap resampling distributions, confidence intervals, and custom tidying and plotting functions.

## Installation

You can install the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("jmgirard/agreement")
```
## Example

Calculate chance-adjusted indexes of categorical agreement for unordered categories

```{r example}
library(agreement)
# Load dataset with 4 raters assigning 12 objects to 5 unordered categories
data(unordered)
print(unordered)
```

```{r}
# Calculate all chance-adjusted indexes for unordered categories
results1 <- cat_adjusted(unordered)
summary(results1, ci = TRUE)
```

```{r}
# Transform results into a tidy data frame
tidy(results1)
```

```{r, fig.height=4}
# Plot the bootstrap resampling distributions with confidence intervals
plot(results1)
```

Calculate chance-adjusted indexes of categorical agreement for ordered categories

```{r}
# Load dataset with 5 raters assigning 20 objects to 4 ordered categories
data(ordered)
print(ordered)
```

```{r}
# Calculate all chance-adjusted indexes for ordered categories (linear weights)
results2 <- cat_adjusted(ordered, weighting = "linear")
summary(results2, ci = TRUE)
```

```{r}
tidy(results2)
```

```{r, fig.height=4}
plot(results2)
```

Calculate category-specific agreement

```{r}
# Calculate category-specific agreement
results3 <- cat_specific(ordered)
summary(results3, ci = TRUE)
```

```{r}
tidy(results3)
```

```{r, fig.height = 4}
plot(results3)
```

Calculate intraclass correlation coefficient for dimensional data with 1 trial

```{r}
# Load dataset with 4 raters rating 15 objects in 1 trial
data(lungfun)
print(lungfun)
```

```{r}
# Calculate average score ICC using Model 1A
results4 <- dim_icc(lungfun, model = "1A", type = "agreement", unit = "average",
                    object = Object, rater = Rater, score = Score, warnings = FALSE)
summary(results4)
```

```{r}
tidy(results4)
```


```{r, fig.height = 4}
plot(results4, intra = FALSE, inter = TRUE)
```

Calculate intraclass correlation coefficient for dimensional data with many trials

```{r}
# Load dataset with 4 raters rating 8 objects in 3 trials
data(lungfun_trials)
print(lungfun_trials)
```

```{r}
# Calculate single score ICC using Model 2A
results5 <- dim_icc(lungfun_trials, Object, Rater, Score, trial = Trial, model = "2", 
                    type = "agreement", unit = "single", warnings = FALSE)
summary(results5)
```

```{r}
tidy(results5)
```

```{r, fig.height = 4}
plot(results5)
```

## Code of Conduct

Please note that the 'agreement' project is released with a
[Contributor Code of Conduct](.github/CODE_OF_CONDUCT.md).
By contributing to this project, you agree to abide by its terms.
